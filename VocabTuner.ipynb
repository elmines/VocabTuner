{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "\n",
    "#Import local modules\n",
    "import os\n",
    "import sys\n",
    "modulesPath = \"scripts\"\n",
    "modulesPath = os.path.abspath(os.path.join(modulesPath))\n",
    "if modulesPath not in sys.path: sys.path.append(modulesPath)\n",
    "from bicorpus import Bicorpus\n",
    "\n",
    "C.cntk_py.set_fixed_random_seed(0)\n",
    "\n",
    "#Model hyperparameters\n",
    "my_dtype = np.float32\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "attention_dim = 128\n",
    "use_attention = True\n",
    "use_embedding = True\n",
    "embedding_dim = 200\n",
    "\n",
    "vocabSize = 30000\n",
    "sourceVocabSize = vocabSize\n",
    "destVocabSize = vocabSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 sequences read.\n",
      "1000 sequences read.\n",
      "1500 sequences read.\n",
      "2000 sequences read.\n",
      "2500 sequences read.\n",
      "3000 sequences read.\n",
      "3500 sequences read.\n",
      "4000 sequences read.\n",
      "4500 sequences read.\n",
      "5000 sequences read.\n",
      "5500 sequences read.\n",
      "6000 sequences read.\n",
      "6500 sequences read.\n",
      "7000 sequences read.\n",
      "7500 sequences read.\n",
      "8000 sequences read.\n",
      "8500 sequences read.\n",
      "9000 sequences read.\n",
      "9500 sequences read.\n",
      "10000 sequences read.\n"
     ]
    }
   ],
   "source": [
    "files = {}\n",
    "\n",
    "sourceTraining = \"corpora/europarl-v7.es-en.es\"\n",
    "destTraining = \"corpora/europarl-v7.es-en.en\"\n",
    "\n",
    "\n",
    "\n",
    "with open(sourceTraining, \"r\", encoding = \"utf-8\") as sourceFile:\n",
    "    sourceLines = sourceFile.readlines()\n",
    "with open(destTraining, \"r\", encoding = \"utf-8\") as destFile:\n",
    "    destLines = destFile.readlines()\n",
    "\n",
    "trainingCorp = Bicorpus(sourceLines, destLines, vocabSize = vocabSize, numSequences = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_lines = trainingCorp.training_lines()\n",
    "sourceWordToI, destWordToI = trainingCorp.getIndexDicts()\n",
    "sourceVocabSize, destVocabSize = len(sourceWordToI), len(destWordToI)\n",
    "\n",
    "seq_start_index = sourceWordToI[Bicorpus.start_token()]\n",
    "seq_end_index = sourceWordToI[Bicorpus.end_token()]\n",
    "seq_start = C.constant(np.asarray([i == seq_start_index for i in range(destVocabSize)], dtype = my_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source and target inputs to the model\n",
    "sourceAxis = C.Axis(\"sourceAxis\")\n",
    "destAxis = C.Axis(\"destAxis\")\n",
    "sourceSequence = C.layers.SequenceOver[sourceAxis]\n",
    "destSequence = C.layers.SequenceOver[destAxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a general sequence-to-sequence model\n",
    "def create_model():\n",
    "    \n",
    "    embed = C.layers.Embedding_dim, name = \"embed\" if use_embedding else identity #Where is \"identity defined?\n",
    "    \n",
    "    with C.layers.default_options(enable_self_stabilization = True, go_backwards = not use_attention):\n",
    "        LastRecurrence = C.layers.Fold if not use_attention else C.layers.Recurrence,\n",
    "        encode = C.layers.Sequential([\n",
    "            embed,\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.For(range(num_layers - 1), lambda: C.layers.Recurrence(C.layers.GRU(hidden_dim))),\n",
    "            LastRecurrence(hidden_dim, return_full_state = True),\n",
    "            C.layers.Label(\"encoded_h\")                                  \n",
    "        ])\n",
    "        \n",
    "    with C.layers.default_options(enable_self_stabilization = True):\n",
    "        stab_in = C.layers.Stabilizer()\n",
    "        rec_blocks = [C.layers.GRU for i in range(num_hidden_layers)]\n",
    "        stab_out = C.layers.Stabilizer()\n",
    "        proj_out = C.layers.Dense(destVocabSize, name = \"out_proj\")\n",
    "        if use_attention:\n",
    "            attention_model = C.layers.AttentionModel(attention_dim, name = \"attention_model\")\n",
    "            \n",
    "        @C.Function\n",
    "        def decode(history, input):\n",
    "            encoded_input = encode(input)\n",
    "            r = history\n",
    "            r = embed(r)\n",
    "            r = stab_in(r)\n",
    "            for i in range(num_layers):\n",
    "                rec_block = rec_blocks[i]\n",
    "                if i == 0:\n",
    "                    if use_attention:\n",
    "                        @C.Function\n",
    "                        def gru_with_attention(dh, x):\n",
    "                            h_att = attention_model(encoded_input.outputs[0], dh)\n",
    "                            x = C.splice(h_att)\n",
    "                            return rec_block(dh, x)\n",
    "                        r = C.layers.Recurrence(gru_with_attention)(r)\n",
    "                    else:\n",
    "                        r = C.layers.Recurrence(rec_block)(r)\n",
    "                else:\n",
    "                    r = C.layers.RecurrenceFrom(rec_block)( *(encoded_input.outputs + (r,)) )\n",
    "            r = stab_out(r)\n",
    "            r = proj_out(r)\n",
    "            r = C.layers.Label(\"out_proj_out\")(r)\n",
    "            return r\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    @C.Function\n",
    "    def model_train(input, labels):\n",
    "        past_labels = C.layers.Delay(initial_state = sequence_start)(labels)\n",
    "        return s2smodel(past_labels, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model used in testing\n",
    "def create_model_greedy(s2smodel):\n",
    "    @C.Function\n",
    "    @C.layers.Signature(InputSequence[C.layers.Tensor[input_vocab_dim]])\n",
    "    def model_greedy(input):\n",
    "        unfold = C.layers.UnfoldFrom(lambda history: s2smodel(history, input) >> C.hardmax,\n",
    "                                    until_predicate = lambda w: w[..., sentence_end_index],\n",
    "                                    length_increase = length_increase)\n",
    "        return unfold(initial_state = sentence_start, dynamic_axes_like = input)\n",
    "    return model_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-77215bea5b81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-77215bea5b81>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "a = [0, 2, 3]\n",
    "x = lambda b: b[..., 3]\n",
    "x(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
