{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "\n",
    "#Import local modules\n",
    "import os\n",
    "import sys\n",
    "modulesPath = \"scripts\"\n",
    "modulesPath = os.path.abspath(os.path.join(modulesPath))\n",
    "if modulesPath not in sys.path: sys.path.append(modulesPath)\n",
    "from bicorpus import Bicorpus\n",
    "\n",
    "C.cntk_py.set_fixed_random_seed(0)\n",
    "\n",
    "#Model hyperparameters\n",
    "my_dtype = np.float32\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "attention_dim = 128\n",
    "use_attention = True\n",
    "use_embedding = True\n",
    "embedding_dim = 200\n",
    "\n",
    "vocabSize = 10000\n",
    "sourceVocabSize = vocabSize\n",
    "destVocabSize = vocabSize\n",
    "\n",
    "numSequences = 10000\n",
    "training_ratio = 3 / 4\n",
    "max_epochs = 1\n",
    "epoch_size = int(numSequences * training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98286 sequences read.\n",
      "196572 sequences read.\n",
      "294858 sequences read.\n",
      "393144 sequences read.\n",
      "491430 sequences read.\n",
      "589716 sequences read.\n",
      "688002 sequences read.\n",
      "786288 sequences read.\n",
      "884574 sequences read.\n",
      "982860 sequences read.\n",
      "1081146 sequences read.\n",
      "1179432 sequences read.\n",
      "1277718 sequences read.\n",
      "1376004 sequences read.\n",
      "1474290 sequences read.\n",
      "1572576 sequences read.\n",
      "1670862 sequences read.\n",
      "1769148 sequences read.\n",
      "1867434 sequences read.\n",
      "1965720 sequences read.\n"
     ]
    }
   ],
   "source": [
    "files = {}\n",
    "\n",
    "sourceTraining = \"corpora/europarl-v7.es-en.es\"\n",
    "destTraining = \"corpora/europarl-v7.es-en.en\"\n",
    "\n",
    "\n",
    "\n",
    "with open(sourceTraining, \"r\", encoding = \"utf-8\") as sourceFile:\n",
    "    sourceLines = sourceFile.readlines()\n",
    "with open(destTraining, \"r\", encoding = \"utf-8\") as destFile:\n",
    "    destLines = destFile.readlines()\n",
    "\n",
    "trainingCorp = Bicorpus(sourceLines, destLines) #, vocabSize = vocabSize, numSequences = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30002\n",
      "30002\n"
     ]
    }
   ],
   "source": [
    "training_lines = trainingCorp.training_lines()\n",
    "sourceW2I, destW2I = trainingCorp.getW2IDicts()\n",
    "sourceI2W, destI2W = trainingCorp.getI2WDicts()\n",
    "sourceVocabSize, destVocabSize = len(sourceW2I), len(destW2I)\n",
    "\n",
    "seq_start_index = destW2I[Bicorpus.start_token()]\n",
    "seq_end_index = destW2I[Bicorpus.end_token()]\n",
    "seq_start = C.constant(np.asarray([i == seq_start_index for i in range(len(destW2I))], dtype = my_dtype))\n",
    "\n",
    "print(sourceVocabSize)\n",
    "print(destVocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source and target inputs to the model\n",
    "sourceAxis = C.Axis(\"sourceAxis\")\n",
    "destAxis = C.Axis(\"destAxis\")\n",
    "sourceSequence = C.layers.SequenceOver[sourceAxis]\n",
    "destSequence = C.layers.SequenceOver[destAxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns a general sequence-to-sequence model\n",
    "def create_model():\n",
    "    embed = C.layers.Embedding(embedding_dim, name = \"embed\") if use_embedding else identity #Where is \"identity defined?\n",
    "    \n",
    "    with C.layers.default_options(enable_self_stabilization = True, go_backwards = not use_attention):\n",
    "        LastRecurrence = C.layers.Fold if not use_attention else C.layers.Recurrence\n",
    "        encode = C.layers.Sequential([\n",
    "            embed,\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.For(range(num_layers - 1), lambda: C.layers.Recurrence(C.layers.GRU(hidden_dim))),\n",
    "            LastRecurrence(C.layers.GRU(hidden_dim), return_full_state = True),\n",
    "            C.layers.Label(\"encoded_h\")                                  \n",
    "        ])\n",
    "    \n",
    "    with C.layers.default_options(enable_self_stabilization = True):\n",
    "        stab_in = C.layers.Stabilizer()\n",
    "        rec_blocks = [C.layers.GRU(hidden_dim) for i in range(num_layers)]\n",
    "        stab_out = C.layers.Stabilizer()\n",
    "        proj_out = C.layers.Dense(destVocabSize, name = \"out_proj\")\n",
    "        if use_attention:\n",
    "            attention_model = C.layers.AttentionModel(attention_dim, name = \"attention_model\")\n",
    "            \n",
    "        @C.Function\n",
    "        def decode(history, input):\n",
    "            encoded_input = encode(input)\n",
    "            r = history\n",
    "            r = embed(r)\n",
    "            r = stab_in(r)\n",
    "            for i in range(num_layers):\n",
    "                rec_block = rec_blocks[i]\n",
    "                if i == 0:\n",
    "                    if use_attention:\n",
    "                        @C.Function\n",
    "                        def gru_with_attention(dh, x):\n",
    "                            h_att = attention_model(encoded_input.outputs[0], dh)\n",
    "                            x = C.splice(x, h_att)\n",
    "                            toReturn = rec_block(dh, x)\n",
    "                            return toReturn\n",
    "                        r = C.layers.Recurrence(gru_with_attention)(r)\n",
    "                        \n",
    "                    else:\n",
    "                        r = C.layers.Recurrence(rec_block)(r)\n",
    "                else:\n",
    "                    r = C.layers.RecurrenceFrom(rec_block)( *(encoded_input.outputs + (r,)) )\n",
    "            r = stab_out(r)\n",
    "            r = proj_out(r)\n",
    "            r = C.layers.Label(\"out_proj_out\")(r)\n",
    "            return r\n",
    "                \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_train(s2smodel):\n",
    "    @C.Function\n",
    "    def model_train(input, labels):\n",
    "        past_labels = C.layers.Delay(initial_state = seq_start)(labels)\n",
    "        return s2smodel(past_labels, input)\n",
    "    return model_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model used in testing\n",
    "def create_model_greedy(s2smodel):\n",
    "    @C.Function\n",
    "    @C.layers.Signature(InputSequence[C.layers.Tensor[input_vocab_dim]])\n",
    "    def model_greedy(input):\n",
    "        unfold = C.layers.UnfoldFrom(lambda history: s2smodel(history, input) >> C.hardmax,\n",
    "                                    until_predicate = lambda w: w[..., sentence_end_index],\n",
    "                                    length_increase = length_increase)\n",
    "        return unfold(initial_state = sentence_start, dynamic_axes_like = input)\n",
    "    return model_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_criterion_function(model):\n",
    "    @C.Function\n",
    "    @C.layers.Signature(input=sourceSequence[C.layers.Tensor[sourceVocabSize]],\n",
    "                        labels=destSequence[C.layers.Tensor[destVocabSize]]) #Should also be \"sourceVocabSize?\"\n",
    "    def criterion(input, labels):\n",
    "        # criterion function must drop the <s> from the labels\n",
    "        postprocessed_labels = C.sequence.slice(labels, 1, 0) # <s> A B C </s> --> A B C </s>\n",
    "        print(\"input =\", input)\n",
    "        print(\"postprocessed_labels =\", postprocessed_labels)\n",
    "        z = model(input, postprocessed_labels)\n",
    "        print(\"z =\", z)\n",
    "        ce = C.cross_entropy_with_softmax(z, postprocessed_labels) #labels)\n",
    "        errs = C.classification_error(z, postprocessed_labels) #labels)\n",
    "        return (ce, errs)\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sequences(sequences, i2w):\n",
    "    return [\" \".join([i2w[np.argmax(w)] for w in s]) for s in sequences]\n",
    "\n",
    "def debug_attention(model, input):\n",
    "    q = C.combine([model, model.attention_model.attention_weights])\n",
    "    #words, p = q(input) # Python 3\n",
    "    words_p = q(input)\n",
    "    words = words_p[0]\n",
    "    p     = words_p[1]\n",
    "    output_seq_len = words[0].shape[0]\n",
    "    p_sq = np.squeeze(p[0][:output_seq_len,:,:]) # (batch, output_len, input_len, 1)\n",
    "    opts = np.get_printoptions()\n",
    "    np.set_printoptions(precision=5)\n",
    "    print(p_sq)\n",
    "    np.set_printoptions(**opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordToOneHot(word, w2i):\n",
    "    vector = np.zeros(len(w2i), dtype = my_dtype)\n",
    "    vector[w2i[word]] = 1\n",
    "    return vector\n",
    "\n",
    "def wordsToOneHot(wordList, w2i):\n",
    "    if type(wordList) == str: wordList = wordList.split(\" \")\n",
    "    return np.stack([wordToOneHot(word, w2i) for word in wordList])\n",
    "\n",
    "def sequencesToOneHot(sequences, w2i):\n",
    "    return np.stack([wordsToOneHot(sequence, w2i) for sequence in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The original had \"vocab\" as a parameter but never used it\n",
    "#The original had CTF readers train_reader and validation_reader\n",
    "def train(sourceW2I, destW2I, s2smodel, max_epochs, epoch_size):\n",
    "    model_train = create_model_train(s2smodel)\n",
    "    criterion = create_criterion_function(model_train)\n",
    "    #model_greedy = create_model_greedy(s2smodel)\n",
    "    \n",
    "    minibatch_size = 72\n",
    "    lr = 0.001 if use_attention else 0.005\n",
    "    learner = C.fsadagrad(model_train.parameters,\n",
    "                         lr = C.learning_rate_schedule([lr]*2 + [lr/2]*3 +[lr/4], C.UnitType.sample, epoch_size),\n",
    "                         momentum = C.momentum_as_time_constant_schedule(1100),\n",
    "                         gradient_clipping_threshold_per_sample = 2.3,\n",
    "                         gradient_clipping_with_truncation = True)\n",
    "    trainer = C.Trainer(None, criterion, learner)                      #\n",
    "    \n",
    "    total_samples = 0\n",
    "    mbs = 0\n",
    "    eval_freq = 100\n",
    "    \n",
    "    C.logging.log_number_of_parameters(model_train) ; print()\n",
    "    progress_printer = C.logging.ProgressPrinter(freq = 30, tag = \"Training\")\n",
    "    \n",
    "    #sparse_to_dense = create_sparse_to_dense(sourceVocabSize)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        mb_num = 0\n",
    "        while total_samples < (epoch + 1) * epoch_size:\n",
    "            startIndex = mbs * minibatch_size\n",
    "            endIndex = startIndex + minibatch_size\n",
    "            \n",
    "            \"\"\"\n",
    "            mb_train = train_reader.next_minibatch(minibatch_size)\n",
    "            trainer.train_minibatch({criterion.arguments[0]: mb_train[train_reader.streams.features],\n",
    "                                     criterion.arguments[1]: mb_train[train_reader.streams.labels]})\n",
    "            \"\"\"\n",
    "            \n",
    "            sourceBatch = sequencesToOneHot(sourceLines[startIndex:endIndex], sourceW2I)\n",
    "            destBatch = sequencesToOneHot(destLines[startIndex:endIndex], destW2I)\n",
    "            trainer.train_minibatch({criterion.arguments[0]: sourceBatch,\n",
    "                                     criterion.arguments[1]: destBatch\n",
    "                                    })\n",
    "            \n",
    "            \n",
    "            \n",
    "            progress_printer.update_with_trainer(trainer, with_metric = True)\n",
    "            \n",
    "            \"\"\"\n",
    "            if mbs % eval_freq == 0:\n",
    "                mb_valid = valid_reader.next_minibatch(1)\n",
    "                e = model_greedy(mb_valid[valid_reader.streams.features])\n",
    "                \n",
    "                #Need to i2w to my own dictionary\n",
    "                #Really, I just need to add a function to my Bicorpus class\n",
    "                print(format_sequence(sparse_to_dense(mb_valid[valid_reader.streams.features]), sourceI2W))\n",
    "                print(\"-->\")\n",
    "                print(format_sequences(e, destI2W))\n",
    "                \n",
    "                if use_attention:\n",
    "                    debug_attention(model_greedy, mb_valid[valid_reader.streams.features])\n",
    "            \"\"\"\n",
    "                    \n",
    "            total_samples += minibatch_size #mb.train[train_reader.streams.labels].num_samples\n",
    "            mbs += 1\n",
    "                \n",
    "    progress_printer.epoch_summary(with_metric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = Input('input', [#, sourceAxis], [30002])\n",
      "postprocessed_labels = Sequence::Slice() -> SequenceOver[destAxis_times_1_minus_1][Tensor[30002]]\n",
      "z = out_proj_out: Composite() -> SequenceOver[destAxis_times_1_minus_1][Tensor[30002]]\n",
      "Training 27648205 parameters in 33 parameter tensors.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'reanudación'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a888039fc4ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourceI2W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestI2W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-d3ace1871988>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(sourceW2I, destW2I, s2smodel, max_epochs, epoch_size)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \"\"\"\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0msourceBatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msourceLines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstartIndex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mendIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msourceW2I\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mdestBatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestLines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstartIndex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mendIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestW2I\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             trainer.train_minibatch({criterion.arguments[0]: sourceBatch,\n",
      "\u001b[1;32m<ipython-input-22-3d6bc6001027>\u001b[0m in \u001b[0;36msequencesToOneHot\u001b[1;34m(sequences, w2i)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordsToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-3d6bc6001027>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordsToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-3d6bc6001027>\u001b[0m in \u001b[0;36mwordsToOneHot\u001b[1;34m(wordList, w2i)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwordsToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-3d6bc6001027>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwordsToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordList\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwordToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msequencesToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-3d6bc6001027>\u001b[0m in \u001b[0;36mwordToOneHot\u001b[1;34m(word, w2i)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwordToOneHot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reanudación'"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train(sourceI2W, destI2W, model, max_epochs, epoch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"model = create_model()\n",
    "print(\"model =\", model)\n",
    "\n",
    "model_train = create_model_train(model)\n",
    "print(\"model_train =\", model_train)\n",
    "criterion = create_criterion_function(model_train)\n",
    "\n",
    "\n",
    "minibatch_size = 72\n",
    "lr = 0.001 if use_attention else 0.005\n",
    "\n",
    "\n",
    "learner = C.fsadagrad(model_train.parameters,\n",
    "                         lr = C.learning_rate_schedule([lr]*2 + [lr/2]*3 +[lr/4], C.UnitType.sample, epoch_size),\n",
    "                         momentum = C.momentum_as_time_constant_schedule(1100),\n",
    "                         gradient_clipping_threshold_per_sample = 2.3,\n",
    "                         gradient_clipping_with_truncation = True)\n",
    "\n",
    "trainer = C.Trainer(None, criterion, learner)\n",
    "\n",
    "esp = sequencesToOneHot([\"él está\", \"la comisión\"], sourceW2I)\n",
    "eng = sequencesToOneHot([\"he is\", \"the commission\"], destW2I)\n",
    "\n",
    "\n",
    "trainer.train_minibatch({criterion.arguments[0]: esp,\n",
    "                         criterion.arguments[1]: eng\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
